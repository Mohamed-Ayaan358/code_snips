{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Workflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFDB8-VqGFeq"
      },
      "source": [
        "> `Almost all ML models follows a basic workflow. This nootebook aims at providing the basic template for carrying out all the steps in common ML projects from defining the problem statement and data preprocessing to all the way up to model development.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUJ92w9Iau9b"
      },
      "source": [
        "## INDEX\n",
        "\n",
        "1. Introduction\n",
        "2. Frame the Problem statement\n",
        "3. Data Collection\n",
        "4. Importing dependencies\n",
        "     - Importing necessary libraries\n",
        "     - Importing the dataset\n",
        "5. Exploratory Data Analysis\n",
        "    - Data Loading and Description\n",
        "    - Data Profiling\n",
        "    - Understanding the Dataset\n",
        "    - Outlier Detection\n",
        "    - Inspecting the dataset\n",
        "    \n",
        "6. Data Preparation\n",
        "    - Handling Missing Values\n",
        "      - Droping Techniques\n",
        "      - Feature Imputation\n",
        "    - Feature Encoding\n",
        "      - One Hot Encoding\n",
        "      - Label Encoding\n",
        "    - Feature Scaling\n",
        "    - Feature Engineering\n",
        "    - Feature Selection\n",
        "      - Dimensionality Reduction\n",
        "      - Feature importance\n",
        "    \n",
        "\n",
        "\n",
        "7. Data Split\n",
        "\n",
        "8. Model Development\n",
        "  - Choosing an Algorithm \n",
        "  - Training the model\n",
        "9. Model Evaluation\n",
        "10. Fine-tuning the Model\n",
        "11. Model Deployment\n",
        "   \n",
        "\n",
        "12. Conclusions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YRsc7AElbGD"
      },
      "source": [
        "\n",
        "##1. Introduction: \n",
        "\n",
        "Many steps in common machine learning problems are repeated iteratively. Above mentions most of the steps, in order, to follow along. The most important step is to figure our whether the concerned problem statemant is a suitable machine leaning problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMIM0eyqjaGz"
      },
      "source": [
        "## 2. Frame the Problem Statement: \n",
        "\n",
        "Problem statement can vary for different ML learnig problems.\n",
        "\n",
        "For Supervised Learning : Choosing the right, strategic decision-making features out of the possible predictors in the dataset to predict the target accurately.\n",
        "\n",
        "For Unsupervised Learning : Analzing patterns out of a data to optimize productions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL968p1lTaVg"
      },
      "source": [
        "## Data Collection\n",
        "Based on:\n",
        "- What kind of problem are we trying to solve?\n",
        "- What data sources already exist?\n",
        "- Any privacy concerns related to the data?\n",
        "- Where should the data be stored?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgwucz93l6zE"
      },
      "source": [
        "##3. Importing dependancies:\n",
        "   - Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5nwvFtCaeuY"
      },
      "source": [
        "# basic libraries or modules\n",
        "\n",
        "import numpy as np                                                 \n",
        "import pandas as pd                                                \n",
        "import pandas_profiling                                            \n",
        "import matplotlib.pyplot as plt                                    \n",
        "import seaborn as sns                                              \n",
        "sns.set()\n",
        "\n",
        "# rest modules can be installed as per need"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7xW9i8ma-7"
      },
      "source": [
        "- Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcIXuZ_61fiz"
      },
      "source": [
        "Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsK5BHc-m_Hj",
        "outputId": "8e452336-bed6-4134-889d-ac3976e94b51"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyQKjmuC1jlD"
      },
      "source": [
        "Importing dataset from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lFBkq4SmOxl"
      },
      "source": [
        "path='..'\n",
        "test_path = '..'\n",
        "my_data=pd.read_excel(path) # in case of excel files\n",
        "my_data = pd.read_csv(path, sep=\" ; \")  # in case of csv files, sep to be mentioned if the values are separated not by commas\n",
        "\n",
        "my_test_data = pd.read_csv(test_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlTIo0kH1sMa"
      },
      "source": [
        " ## 4. Exploratory Data Analysis\n",
        "  - Data Loading and Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2k2JWPkWpot"
      },
      "source": [
        "my_data.head(10)  # prints the first 10 entries of the dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imnAHKibXbPn"
      },
      "source": [
        "my_data.tail(8)  # prints the last 8 entries of the dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wAhn6zu6pjt"
      },
      "source": [
        "## 5. Data profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C-nO4IdosZC"
      },
      "source": [
        "To understand the Data structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzSjzwXJYfVp"
      },
      "source": [
        "my_data.shape   # (no. of rows, no. of cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_SeQdnraGno"
      },
      "source": [
        "my_data.columns   # array of all features in our dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY1OU-thW06k"
      },
      "source": [
        "my_data.info(verbose = True)  # This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B71oxxhAW1G3"
      },
      "source": [
        "my_data.describe()  # This method prints statistical information about the numerical feature of a DataFrame "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWIH9ykHcXEy"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = pandas_profiling.ProfileReport(my_data)  # or\n",
        "profile = pandas_profiling.ProfileReport(my_data.sample(n=10000), minimal =True)  # for simplified report with firat 10000 entries\n",
        "profile.to_file(outputfile=\"my_data_before_preprocessing.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhSilyZqcccB"
      },
      "source": [
        "my_data.isnull().sum()   # series of null value count of all the features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lfgxFZs3AIk"
      },
      "source": [
        "Information of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgxNo8G3x2ZS"
      },
      "source": [
        "##Dataset info\n",
        "\n",
        "- Number of variables\t- \n",
        "- Number of observations - \n",
        "\n",
        "###Variables types\n",
        "\n",
        "- Structured or Unstructured - \n",
        "- Numeric\t- \n",
        "- Categorical\t- \n",
        "- Boolean\t- \n",
        "- Time Series\t- \n",
        "- Ordinal - \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-oV3aZcysue"
      },
      "source": [
        "There are total ... columns in the dataset namely\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJNwHYVD5cjj"
      },
      "source": [
        "###To find the unique values calumns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMVejPldc9_z"
      },
      "source": [
        "categorical_features=[feature for feature in my_data.columns if my_data[feature].dtypes=='O']\n",
        "print(categorical_features)\n",
        "my_data[categorical_features_in_train].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayQh5lztpz9R"
      },
      "source": [
        "for feature in categorical_features:\n",
        "    print('The feature is {} and number of categories are {}'.format(feature,len(my_data[feature].unique())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLdpK7MidTyv"
      },
      "source": [
        "low_card_categorical_feature = [col for col in categorical_features if my_data[col].nunique()<10 ]\n",
        "print(\"Low cardinality categorical feature\" : , low_card_categorical_feature )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kDzirHVdLet"
      },
      "source": [
        "numerical_features = [feature for feature in my_data.columns if my_data[feature].dtypes != 'O']\n",
        "\n",
        "print('Number of numerical variables: ', len(numerical_features))\n",
        "\n",
        "# Preview of the numerical variables\n",
        "my_data[numerical_features].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvqQLnNSrjWp"
      },
      "source": [
        "total_cols = low_card_categorical_feature + numerical_features\n",
        "my_data_reduced = my_data[total_cols].copy()\n",
        "my_data_reduced.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUR-XzGpdSZl"
      },
      "source": [
        "for cols in my_data.columns: \n",
        "  print(f'The unique values in {col}: {my_data[col].unique()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDFAaaQjc3sd"
      },
      "source": [
        "for cols in my_data.columns: \n",
        "  print(f'No. of unique values in {col}: {my_data[col].nunique()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kMEshBQeVD5"
      },
      "source": [
        "for cols in my_data.columns: \n",
        "  print(f'No. of unique values in {col}: {my_data[col].nunique()}')    #returns object containing counts of unique values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geGC6l_W75fh"
      },
      "source": [
        "## Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hmwb0_J7vq4"
      },
      "source": [
        "for cols in my_data.columns:\n",
        "  my_data.boxplot(column=cols, by='quality', grid=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx7OWsCN38E7"
      },
      "source": [
        "#Data Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "VDh1kpUzzcZs",
        "outputId": "91c11223-dce3-4bc4-9a4a-f26d1daceb79"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.palplot(sns.color_palette('PuOr')) #Purpole to orange colors\n",
        "pal=sns.color_palette('PuOr',6) #To print 6 shades from purple to orange\n",
        "pal.as_hex() #set hex code values for colors\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(['seaborn'])\n",
        "sns_colors=['#c6690c','#664697'] #orange purple hex codes\n",
        "sns.set_palette(sns_colors) #set the palette as sns_colors\n",
        "sns.palplot(sns.color_palette(sns_colors)) #plot the color codes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAABlCAYAAABpyxuAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADD0lEQVR4nO3asW5bdRjG4e9/bCfCjshQ1VKZ2DuAGHIFSEyds7N17AVUtbgChizcQKWsTFTiGhBCXABThGxYCq0USnK4ASTwiaPz2n2e2Sf69MrSTz5K6/u+LwAgTjf2AQDAvxNpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCTe/ycH97Uz+8+KSuf/9lR+cctuMHH9dnX/1U775/VvV2PfY5+2O+rNnnX9ft5seqm+uxr9kPk+PqHn5aV1ev6++b27Gv2RvTSVcfPfqwvvv253rz519jn7MXFidH9cWTx/X8y5f1269/jH3OXvnm1dP//Mydfkm3blLT+eld/sR7ZTo/rda6arP52KfslTabV2utqk3GPmV/tEm11qrr2tiX7JWua9W6VrMj37X/a3Y0qW7S1Qcnx2OfcpC87gaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCiTQAhBJpAAgl0gAQSqQBIJRIA0AokQaAUCINAKFEGgBCtb7v+yEPrtfrury8rPPz81oul7u+6yDZbBi7bc9mw9hteza7X4N/SW82m7q4uKjNZrPLew6azYax2/ZsNozdtmez++V1NwCEEmkACCXSABBqslqtVkMfXiwWdXZ2VovFYocnHTabDWO37dlsGLttz2b3Z/B/dwMA98vrbgAIJdIAEEqkASCUSANAKJEGgFAiDQChRBoAQok0AIT6B4YQiUoxinAMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 600x100 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAABhCAYAAACgcPGxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABgklEQVR4nO3WsUkDUBhG0ZcohEjAVVzH1solxMIltLF1ARuncAVLC63EwkKfK4SAhBvPqV/xFZeft5hzzgFBy30PgF2JlyzxkiVessRLlnjJEi9Z4iVLvGQdb/tw/nyP5+uz8fX+8odzDsvj5+24uT8fVxcP4+31Y99zMu6eLrd6t/XlXSyPxvHJ6c6D/qP1ZjWWR8ux3qz2PeUg+TaQJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEvWYs459z0CduHykiVessRLlnjJEi9Z4iVLvGSJlyzxkvULwosaHSJ18LYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 200x100 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UauQgWgcPlh"
      },
      "source": [
        "my_data.hist(bins=20 ,figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt8d3iw38Gol"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FDpDx4ZmYX0"
      },
      "source": [
        "## Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q85LwjO_4Pnf"
      },
      "source": [
        "# Get names of columns with missing values\n",
        "cols_with_missing = [col for col in X_train.columns\n",
        "                     if my_data[col].isnull().any()]\n",
        "\n",
        "# Drop columns in training and validation data\n",
        "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
        "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPab2pRQmstI"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Imputation\n",
        "my_imputer = SimpleImputer(strategy = 'median')   # there can be other strategy like most_frequent or mean \n",
        "imputed_data = pd.DataFrame(my_imputer.fit_transform(my_data))\n",
        "\n",
        "# Imputation removed column names; put them back\n",
        "imputed_data.columns = my_data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-er_7g6unVYp"
      },
      "source": [
        "# Make copy to avoid changing original data (when imputing)   # Next, we impute the missing values, while also keeping track of which values were imputed.\n",
        "data_plus = X_train.copy()\n",
        "\n",
        "# Make new columns indicating what will be imputed\n",
        "for col in cols_with_missing:\n",
        "    data_plus[col + '_was_missing'] = data_plus[col].isnull()\n",
        "\n",
        "# Imputation\n",
        "my_imputer = SimpleImputer()\n",
        "imputed_data_plus = pd.DataFrame(my_imputer.fit_transform(data_plus))\n",
        "\n",
        "# Imputation removed column names; put them back\n",
        "imputed_data_plus.columns = data_plus.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3kYQMNxsXQq"
      },
      "source": [
        "## Feature Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlNBDE1ysb9n"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(my_data[low_card_categorical_feature]))\n",
        "\n",
        "OH_cols_train.index = my_data.index\n",
        "\n",
        "num_X_train = my_data.drop(categorical_col, axis=1)\n",
        "\n",
        "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t9_42wnt4WI"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "object_cols =  categorical_features\n",
        "# Make copy to avoid changing original data \n",
        "label_X_train = my_data.copy()\n",
        "\n",
        "# Apply ordinal encoder to each column with categorical data\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "label_X_train[object_cols] = ordinal_encoder.fit_transform(my_data[object_cols])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LagpgBCVtkJg"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5eB5rRltaHo"
      },
      "source": [
        "feature_scale=[feature for feature in OH_X_train.columns ]\n",
        "from sklearn.preprocessing import StandardScaler  # Standardize features by removing the mean and scaling to unit variance.\n",
        "np.random.seed(42)\n",
        "\n",
        "scaler=StandardScaler()\n",
        "scaler.fit(OH_X_train[feature_scale])\n",
        "scaler.transform(OH_X_train[feature_scale])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1tqwgMYu-MW"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler  #Transform features by scaling each feature to a given range or default in between 0 to 1\n",
        "\n",
        "scaler=MinMaxScaler()\n",
        "scaler.fit(OH_X_train[feature_scale])\n",
        "scaler.transform(OH_X_train[feature_scale])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhjvrUGSvpA1"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Feature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. A feature is a property shared by independent units on which analysis or prediction is to be done. Features are used by predictive models and influence results. Now, a single feature can be used independent or can be combined with other available features for better feature generation. It depends on a use case and problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzmaNdlsvrXB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9mOgE-qwYS8"
      },
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1fzYpKOwp2y"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)   # here, the code projects the original data which is total dimensional into 2 dimensions.\n",
        "principalComponents = pca.fit_transform(my_data)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Q17mJSyINL"
      },
      "source": [
        "`PCA` is one of the most common ways of feature reduction and extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv-MtalpqdtE"
      },
      "source": [
        "## Data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyASHUGCqbvb"
      },
      "source": [
        "#Seprating features and labels (X and y)\n",
        "X = my_data.drop(['target'],axis=1)   #target is the target column\n",
        "y = df['target']\n",
        "\n",
        "\n",
        "#Splitting data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state=42)  # we can specify a different test_size\n",
        "\n",
        "print(X_train_full.shape , X_valid_full.shape, y_train.shape, y_valid.shape)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZSIFuxzH5-"
      },
      "source": [
        "## Feature Importance (Post-modelling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGdMd6U0yIfm"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif  \n",
        "selector = SelectKBest(f_classif, k=4)  # for 4 best features\n",
        "selector.fit(X_train_full, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbECfqsGyotE"
      },
      "source": [
        "cols = selector.get_support(indices=True)\n",
        "cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrSjLS3tyy4k"
      },
      "source": [
        "selector.scores_  # We can also verify by the F-scores of the features from the above cell."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uOrgHFLy-1o"
      },
      "source": [
        "X_train_s = X_train_full.iloc[:,cols]   ## chosing our best 4 features\n",
        "X_valid_s = X_valid_full.iloc[:,cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaYnnilg2-Ap"
      },
      "source": [
        "X_train_final = \n",
        "X_valid_final ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc-pySm_z0CF"
      },
      "source": [
        "## Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20G2-bA96w_a"
      },
      "source": [
        "There are numerous model choices for us. We may regression or classification choose model as per our problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYVOufs2cPQ"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor   # regressor SGD\n",
        "model = SGDRegressor(random_state=1)\n",
        "model.fit(X_train_final,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOJxNiT14XSP"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor      # tree algorithm (regressor)\n",
        "\n",
        "model = DecisionTreeRegressor(random_state=0, max_depth= 10)\n",
        "model.fit(X_train_final,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V1poUlxzzNO"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor   # ensemble modeling  (regressor)\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Function for comparing different approaches\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "model.fit(X_train_final, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JWo7aOf0Auk"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76GbcT5T1zwK"
      },
      "source": [
        "preds = model.predict(X_valid_final)\n",
        "\n",
        "#Evaluating our estimator\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "score = mean_absolute_error(y_valid, preds)            # common metrices for regression model\n",
        "print(\"MAE:\",score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yBGPjwu0xDf"
      },
      "source": [
        "from sklearn import metrics                                   # common metrices for classification model\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, preds))\n",
        "print(\"Recal:\" , metrics.recall_score(y_test, preds))\n",
        "print(\"precision:\" , metrics.precision_score(y_test, preds))\n",
        "print(\"f1_score:\" , metrics.f1_score(y_test, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6x5HHvcz5QL"
      },
      "source": [
        "## Fine-Tuning the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f_EPq67z9ES"
      },
      "source": [
        "from xgboost import XGBRegressor   # boosting algorithm (regressor)\n",
        "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
        "\n",
        "model =  XGBRegressor()\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model,\n",
        "            param_distributions=params,\n",
        "            cv=5, n_iter=5,\n",
        "            scoring = 'neg_mean_absolute_error',\n",
        "            verbose = 5, \n",
        "            return_train_score = True,\n",
        "            random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_2DD4GL0phb"
      },
      "source": [
        "predictions = my_model.predict(OH_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsm07MTC0_6d"
      },
      "source": [
        "random_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnXV-6jd1BCg"
      },
      "source": [
        "random_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPaHxOaX1W-0"
      },
      "source": [
        "\n",
        "my_model=XGBRegressor(....)  # replace the .... with the best parameters given by randomized search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4_4knEl6OtN"
      },
      "source": [
        "We acn reevaluate our model and iterate our training here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz5SW8eW6VpH"
      },
      "source": [
        "## Model Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iv01teG99C0"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Take the space to explain the motive of your final model."
      ]
    }
  ]
}